ğŸ“Œ Detailed Analysis Report â€“ Mistral-7B-v0.1 Fine-Tuned on GSM8K

ğŸ“ Overview
Model: mistralai/Mistral-7B-v0.1
Dataset: openai/gsm8k (Mathematical Reasoning)
Epochs: 3
Cost: $2.075
Fine-tuning Method: LoRA (Low-Rank Adaptation)
Prompt Configuration Used:
Instruction: {question}
Response: {answer}
Data Split: 90% Train, 10% Validation
Training Framework: PyTorch 2.6.0
This experiment aimed to further improve Mistral-7Bâ€™s mathematical reasoning skills using GSM8K, a dataset known for challenging grade-school math problems.
The previous 2-epoch run showed promising results, and this 3-epoch version ensures deeper learning.

ğŸ“Š Key Finetuning Metrics
ğŸ”¹ Training & Validation Loss Improvement
Metric	Value
Train Loss Improvement	             65.83% âœ…
Validation Loss Improvement	         26.07% âœ…
Mean Token Accuracy (Final Epoch)   ~83.98% âœ…

ğŸ”¹ Observations from Training Logs
âœ… Training loss improved significantly (indicating deeper learning).
âœ… Validation loss showed even better improvements compared to 2 epochs (model generalizing well).
âœ… Higher token accuracy (~83.98%) than the 2-epoch version (~81.3%).
âœ… Still no signs of overfitting, meaning model is robust.
âœ… Learning curve suggests further improvement was achieved with 3 epochs.

ğŸ”¬ Insights from Model Analysis

ğŸ“Œ Word Cloud Analysis
Most frequent words: Mathematical terms (e.g., "sum," "total," "difference," "product").
Indicates that the model has captured core mathematical reasoning patterns.

ğŸ“Œ UMAP & KMeans Clustering
The model clustered similar problem types together, meaning it understands different problem-solving approaches.

ğŸ“Œ Outlier Detection
Some rarely occurring math problems were identified as anomalies.
Example: Unique multi-step algebra problems.

ğŸ›  What Worked Well?
âœ” Further reduced loss compared to the 2-epoch version.
âœ” Higher token accuracy (~83.98% vs. 81.3%).
âœ” Mathematical problem-solving skills improved significantly.
âœ” Model retained efficiency while fine-tuning at a low cost of $2.075.