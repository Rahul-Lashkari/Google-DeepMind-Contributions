ğŸ“Œ Fine-Tuning Analysis Report: Google/Gemma-2B on Google/BoolQ
Google Summer of Code (GSoC 2025) â€“ Contributions to Google DeepMind
ğŸ“… Date: 17-03-2025
ğŸ‘¤ Author: Rahul Lashkari + AI
ğŸ› ï¸ Objective: Fine-tune Gemma-2B on Google/BoolQ (Yes/No Question Answering) and analyze results for submission as Contributions to Google DeepMind (GSoC 2025).

ğŸ“Œ 1. Fine-Tuning Overview
Model: google/gemma-2b
Dataset: google/boolq (Boolean Question Answering)
Fine-Tuning Type: Instruction Fine-Tuning
Epochs: 3
Cost: $7.975
Learning Rate: 0.0002
LoRA Parameters:
lora_r: 8
lora_alpha: 16
lora_dropout: 0.1
lora_bias: "none"
Gradient Accumulation Steps: 1
Early Stopping Patience: 5
Data Split: 90% Train / 10% Validation

ğŸ“Œ 2. Key Training Metrics & Observations
ğŸ”¹ Training & Validation Loss Improvement
âœ… Train Loss Decrease: 60.92%
âœ… Validation Loss Decrease: 6.25%

ğŸ“‰ Interpretation:
The model significantly reduced training loss, confirming successful pattern recognition.
Validation loss decrease (6.25%) suggests limited generalization improvement.
Further tuning (extra epochs or dataset expansion) might be beneficial.

ğŸ”¹ Learning Curve & Further Training Potential
ğŸ“ˆ Key Observations:
Final training steps still showed loss reduction (~7.8%), meaning the model was still improving when training stopped.
Recommendation: Running one additional epoch could further enhance generalization.

ğŸ”¹ Model Performance on Token Accuracy
âœ… Final Mean Token Accuracy: 64.8%
âœ… Started at: 47.8% â†’ Reached: 64.8%

ğŸ“Œ Interpretation:
The model effectively learned the yes/no question-answering format.
Further improvement is possible with fine-tuning on similar datasets (e.g., Natural Questions, FEVER).

ğŸ“Œ 3. Model Insights â€“ Outliers & Anomalies Found
ğŸš¨ Unusual Question Categories Detected:
1ï¸âƒ£ Entertainment Queries:
"Was 'Breaking Bad' renewed for a sixth season?"
"Is Stranger Things based on a true story?"
2ï¸âƒ£ Biological and Scientific Queries:
"Can a woman get pregnant while already pregnant?"
"Is water an element or a compound?"

ğŸ“Œ Why This Matters?
Entertainment-based BoolQ queries were answered more confidently.
Scientific questions had a higher uncertainty margin (confidence ~54%), meaning the model still struggled with factual ambiguity.

ğŸ“Œ 4. Final Thoughts â€“ GSoC Impact
âœ… Fine-Tuning on BoolQ was successful!
âœ… Gemma-2B improved in Yes/No Question Answering but may benefit from:
Additional fine-tuning on fact-based datasets (e.g., FEVER, Natural Questions).
Expanding BoolQ training data with hard negatives to reduce factual uncertainty.